神经网络
=======

## 单神经元网络
神经网络算法领域最初是被对生物神经系统建模这一目标启发，但随后与其分道扬镳称为一个工程问题，并在机器学习领域取得很好的效果。然而，讨论将还是从对生物系统的一个高层次的简略描述开始，毕竟神经网络是从这里得到启发并发展的。

### 生物动机与连接
大脑的基本组成单元是**神经元**。人类神经系统拥有大量的神经元且彼此间通过**突触**进行连接。这里不过多的进行介绍。根据神经元结果，构建了神经元的计算模型，如下图所示：
{% asset_img neuron_model.jpg %}
在上图中，沿着轴突传播的信号($x_0$)将基于突触的强度($w_0$)与其他神经元的树突进行乘法交互($w_0x_0$)。其观点是，突触的强度($w$)是可以学习的并且可以控制一个神经元对另一个神经元的影响强度以及方向。在基本模型中，输入将信号传递到胞体内，信号在胞体内进行相加。如果最终之和高于某个阈值，那么神经元将会激活，向其轴突输出一个峰值信号。在计算模型中，我们假设峰值信号的准确时间点不重要，是激活信号频率在交流信息。基于这个速率的编码观点，将神经元的激活率建模成**激活函数**，表示轴突上激活信号的频率。激活函数通常使用**sigmoid函数$\sigma$**，该函数输入实数值，然后将输入值压缩到0-1之间。
下面给出一个神经元前向传播的实例：
``` python
class Neuron(object):
    # ...
    def forward(inputs):
        """ assume inputs and weights are 1-D numpy arrays and bias is a number. """
        cell_body_sum = np.sum(inputs * self.weights) + self.bias
        firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum))    # sigmoid activation function
        return firing_rate
```
每个神经元都是对它的输入和权重进行点积，然后加上偏差，最后使用非线性函数(激活函数)。上面程序中使用了sigmoid函数$\sigma(x) = ^1/_{(1 + e^{-x})}$。
**粗糙模型**：要注意这个对于生物神经元的建模是非常粗糙的。在实际中，有很多不同类型的神经元，每种都有不同的属性。生物神经元的树突可以进行复杂的非线性计算。突触并不就是一个简单的权重，它们是复杂的非线性动态系统。很多系统中，输出的峰值信号的精确时间点非常重要，说明速率编码的近似是不够全面的。鉴于所有这些已经介绍和更多未介绍的简化，如果你画出人类大脑和神经网络之间的类比，有神经科学背景的人对你的板书起哄也是非常自然的。

### 单神经元作为线性分类
但神经网络的前向计算的数学公式可能看起来很熟悉，就像线性网络中一样，神经元有能力对线性区域表现出倾向(激活函数接近1)或者不倾向(激活函数接近0)的能力。我们通过计算神经元输出的损失函数，可以将其变成一个线性分类器：
1.  **二分类Softmax分类器**，我们可以将$\sigma (\sum_i{w_ix_i} + b)$看成是一个分类的概率$P(y_i=1|x_i;w)$。其他分类的可能性为$P(y_i = 0|x_i;w) = 1 - P(y_i = 1|x_i; w)$，因为总概率要满足和为1。根据上面的解释，我们可以使用交叉熵的损失将其最优化为二分类的Softmax分类器(也就是逻辑回归)。因为Sigmoid函数将输出限定在0-1之间，所以分类器做出的预测的基准是神经元的输出是否带0.5。
2.  **二分类SVM分类器**，在神经元的输出增加一个最大边界折页损失(max-margin hinge loss)函数，那么就变成了二分类的SVM分类器。
**对正则化的理解**，在SVM/Softmax的例子中，正则化损失从生物角度可以看成是逐渐遗忘，因为它的效果是让所有的突触权重$w$在参数更新过程中逐渐向着0变化。
>一个单独的神经元可以用来实现一个二分类的分类器，但是无法实现非线性的网络。

## 激活函数

### Sigmoid
{% asset_img sigmoid.jpg %}
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$
将输入的实数值“挤压”到0-1的范围内。更确切的说，极小的负数变成0，极大的正数变成1。sigmoid函数在以前是非常常用的，这是因为它对神经元的激活频率有良好的解释：从完全不激活(0)到求和后最大频率处的完全激活(1)。但是现在很少使用sigmoid函数是一下两个缺点导致：
1.  **Sigmoid函数饱和使梯度消失**：当神经元的激活在接近0或者1的时候会饱和，梯度几乎会变成0。如果梯度过小，那么反向传播到数据就几乎可以忽略不记了，最终导致整个网络几乎不学习。
2.  **Sigmoid函数的输出不是零中心的**：如果输入神经元的数据总是正数，那么关于$w$的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数。这将会导致梯度下降权重更新时出现`z字形`下降，导致学习缓慢。
### Tanh
{% asset_img tanh.jpg %}
tanh将实数值压缩到[-1, 1]之间，和sigmoid函数一样**存在饱和问题**，但是和sigmoid不同的是，它的输出是零中心的。因此在实际使用中，tanh更受欢迎。同时，tanh神经元可看作是简单放大的sigmoid神经元：$tanh(x) = 2\sigma (2x) - 1$。
### ReLU
{% asset_img relu.jpg %}
$$ f(x) = \max(0, x) $$
relu是一个关于0阈值的函数，有以下特点：
1.  相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有着巨大的加速作用，这是由它的线性和非饱和导致的。[参考](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)。
2.  sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU仅仅通过对矩阵进行简单的阈值计算即可得到。
3.  在训练的时候，ReLU单元比较脆弱并且可能‘死掉’。当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，这种状态下神经元将无法被其他任何数据点再次激活。也就是说，ReLU单元在训练中将不可逆转的死亡，将会导致数据多样化的丢失。（学习率设置的过高，40%的神经元都会死掉，因此可以降低学习率来降低发生的概率）。
### Leaky ReLU
{% asset_img alexplot.jpg %}
$$ f(x) = \mathbb{1}(x < 0)(\alpha x) + \mathbb{1}(x >= 0)(x) $$
Leaky ReLU是为了解决ReLU死亡的问题。正如上式中，Leaky ReLU中当$x < 0$时，会给出一个很小的梯度值。这个公式性能不错，但是不是很稳定。2015年何凯明在论文中介绍了一个新的方法[PReLU](http://arxiv.org/abs/1502.01852)，将负区间上的斜率当作每个神经元中的一个参数，但是并没有证明能够适用于所有的任务。
### Maxout
Maxout是对ReLU和leaky ReLU的一般化归纳，公式为：$\max(w_1^Tx + b_1, w_2^Tx + b_2)$。ReLU和Leaky ReLU都是这个公式的特殊情况。这样，Maxout就能具有ReLU的所有优点，而没有其缺点。但是它的每个神经元的参数数量增加了一倍，导致整体参数的数量激增。

> 使用ReLU函数，要注意设置好学习率或者是监控网络中单元的死亡率。如果比较困扰，那么就直接使用Leaky Relu或者是Maxout。不要继续使用sigmoid了，可以尝试一下tanh，但是效果并不如Relu/Maxout好。

## 神经网络结构

### 灵活的组织
**将神经网络作为神经元使用图片进行展示**：神经网络可以看成是一系列的神经元组成的，神经元之间使用无环图的方式进行连接。也就是说一部分神经元的输出是另一部分神经元的输入。在网络中不允许出现循环，以防止在前向传播的时候出现无限循环的情况。不像真正的神经元，神经网络中的神经元经常是分层的。普通的神经网络中最常用的是全连接层，用来对前后两层的神经元的成对连接，而在全连接层内部的神经元之间没有任何连接。下面两个图便是使用全连接层的神经网络模型：
{% raw %}
<div>
  <img src="neural_net.jpg" width="40%" style="display: inline-block"/>
  <img src="neural_net2.jpg" width="55%" style="display: inline-block; border-left: 1px solid black;" />
</div>
{% endraw %}


1.  **命名规则**：对于N层神经网络时，并不会将输入层计入在内。因此单层神经网络没有隐藏层，输入直接映射到输出上。在这种命名规则上，人们通常成逻辑回归或SVM为简单的单层神经网络的特例。也有人会用*Artificial Neural Networs*(ANN)或者是*Multi-Layer Perceptrons*(MLP)来替代神经网络。
2.  **输出层**：不像神经网络中的其他层，输出层通常没有激活函数(或者你认为输出层有一个线性相等的激活函数)。这是因为输出层大多用于表示评分，因此时任意值的实数，或者某种实数值的目标数。
3.  **确定网络尺寸**：用来衡量神经网络尺寸的标准主要有两个：一个是神经元的个数，另一个是参数的个数。例如：上图的第一个网络有4 + 2 = 6个神经元，[3 x 4] + [4 x 2] = 20个权重，还有4 + 2 = 6个偏置，共26个学习参数。第二个网络有4 + 4 + 1 = 9个神经元，[3 x 4] + [4 x 4] + [4 x 1] = 32个权重，4 + 4 + 1 = 9个偏置，共41个可学习的参数。

### 前向传播计算
将神经网络进行分层的主要原因是能够简单高效的使用矩阵向量操作。用上面图片中的三层神经网络为例子，输入为[3 x 1]的向量。所有处于同一层的权重可以储存在同一个矩阵中，例如第一个隐藏层的权重`$W_1$`的大小为[4 x 3]，偏置储存在向量`$b_1$`中，大小为[4 x 1]。每个单独的神经元的权重时`$W_1$`的一行，因此矩阵乘法`np.dot(W1, x)`就能计算该层中所有神经元的激活数据。类似的，`$W_2$`储存这第二个隐藏层的权重，为[4 x 4]的矩阵。`$W_3$`为[1 x 4]的矩阵，作为输出层。完整的3层的神经网络只需要经过3次矩阵的乘法即可，代码如下：
``` python
# formard-pass of a 3-layer neural network
f = lambda x: 1.0 / (1.0 + np.exp(-x))  # activation function(sigmoid).
x = np.random.randn(3, 1)               # random input vector of three numbers
h1 = f(np.dot(W1, x) + b1)
h2 = f(np.dot(W2, h1) + b2)
out = np.dot(W3, h2) + b3
```
在上面的代码中，W1，W2，W3，b1，b2，b3都是网络中可以学习的参数。注意x并不是一个单独的列向量，而可以是一个批量的训练数据（其中每个输入样本将会是x中的一列），所有的样本将会被并行化的高效计算出来。注意神经网络最后一层通常是没有激活函数的（例如，在分类任务中它给出一个实数值的分类评分）。
>全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数。

### 表达能力
理解具有全连接层的神经网络的一个方式是，可以认为它们定义了一个由一系列函数组成的函数蔟，网络的权重就是每个函数的参数。那么该函数蔟的表达能力如何？存在不能被神经网络表达的函数吗？
现在看来，拥有至少一个隐藏层的神经网络是一个通用的近似器。给出任意联系的函数$f(x)$和任意$epsilon > 0$，存在拥有一个隐藏层的神经网络$g(x)$使得$\forall x, \mid f(x) - g(x) \mid < \epsilon$。也就是说神经网络可以近似任何连续函数。
既然一个隐藏层就能近似任意函数，那么为什么还要构建深层的网络呢？答案是虽然一个两层的网络在数学理论上能完美的近似任何连续函数，但实际运用中效果相对较差。在一个维度上，虽然以$a, b, c$为参数向量的和函数$g(x) = \sum_i c_i \mathbb{1}(a_i < x < b_i)$也是通用的近似器，但是谁也不会建议在机器学习中使用这个函数公式。神经网络在实践中非常好用，是因为他们不仅表达出的函数不仅平滑，而且对于数据的统计特性有很好的拟合。同时，神经网络通过最优化算法能比较容易地学习到这个函数。类似的，虽然在理论上深层网络和单层网络的表达能力是一样的，但就实际运用来说，深层网络效果比单层网络好。

### 设置层的数量和尺寸
在面对一个具体问题时该如何确定网络结构呢，该使用多少隐藏层，每个层的尺寸为多大？
首先，要知道当我们增加层的数量和尺寸时，网络的容量上升了。即神经元合作表达出许多复杂的函数，所以表达函数的空间得以增加。例如我们使用不同的神经网络进行训练，每个网络只有一个隐藏层，但是每个隐藏层的神经元数目不同，效果如下图：
{% asset_img layer_sizes.jpg %}
在上图中，可以看到更多的神经元的神经网络可以表达出更复杂的函数，然而这既是优势也是不足。优势是**可以分类更复杂的数据**，不足时**可能造成对训练数据的过拟合**。过拟合是网络对数据中的噪声有很强的拟合能力，而没有重视数据间的潜在的基本关系。相反，有3个隐藏层的神经元模型的表达能力只能用比较宽泛的方式去分类数据，这样能获得更好的**泛化**能力。

不要减少网络神经元数目的主要原因在于小网络更难使用梯度下降等局部方法来进行训练：虽然小型网络的损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。因为神经网络是非凸的，就很难从数学上研究这些特性。在实际中，你将发现如果训练的是一个小网络，那么最终的损失值将展现出多变性：某些情况下运气好会收敛到一个好的地方，某些情况下就收敛到一个不好的极值。从另一方面来说，如果你训练一个大的网络，你将发现许多不同的解决方法，但是最终损失值的差异将会小很多。这就是说，所有的解决办法都差不多，而且对于随机初始化参数好坏的依赖也会小很多。
重申一下，正则化强度是控制神经网络过拟合的好方法。看下图结果：
{% asset_img reg_strengths.jpg %}
> 不同正则化强度的效果：每个神经网络都有20个隐层神经元，但是随着正则化强度增加，它的决策边界变得更加平滑。

*需要记住的是：不应该因为害怕出现过拟合而使用小网络。相反，应该进尽可能使用大网络，然后使用正则化技巧来控制过拟合。*

## 数据处理和模型选择

### 数据预处理
原始数据`X`，一般假设大小为`[N x D]`，`N`代表数据的数量，`D`代表每个数据的维度。通常有以下三种常见的处理方式：
1.  **均值减法(Mean Subtraction)**是最常见的数据处理方式。它对数据中每个独立特征减去平均值，从几何上可以理解为在每个维度上将数据云的中心都迁移到原点。可以使用‘Numpy’包进行数据处理：`X -= np.mean(X, axis=0)`。而对于图像来说，更常用的是对所有的像素都减去一个值，使用`X -= np.mean(X)`实现，也可以在每个通道上分别进行操作。
2.  **归一化(Normalization)**是指将数据的所有维度都归一化，使其数值范围都近似相等。通常有两种方式能够实现归一化：
    * 先对数据进行零归一化，然后在每个维度都除以它的标准差： `X /= np.std(X, axis=0)`。
    * 在每个维度都做归一化，是每个维度上的最大值和最小是分为是1、-1。
在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。
3.  **PCA和白化(Whitening)**是另一种预处理方式，在这种处理中，先对数据进行零对称差分处理，然后计算协方差矩阵，协方差矩阵能够展示数据中的相关性结构。
``` python
X -= np.mean(X, axis=0)
cov = np.dot(X.T, X) / X.shape[0]
```
数据协方差矩阵的第(i, j)个元素是数据第i个和第j个维度上的*协方差*，具体来说，该矩阵的对角线上的元素都是方差。还有，协方差矩阵是对称和半正定的。我们可以对数据协方差矩阵进行奇异值分解运算：
``` python
U, S, V = np.linalg.svd(cov)
```
其中，U的列是特征向量，S是奇异值的一维数组。为了去除数据的相关性，将已经零对称差分处理过的原始数据头型到特征基准上：
``` python
Xrot = np.dot(X, U)
```
注意，U的列是标准正交向量的集合，所以可以把它们看作是标准正交基向量。因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。`Xrot`的协方差矩阵是对角对称的。`np.linalg.svd`的一个良好性质是在它的返回值`U`中，特征向量按照特征值的大小排列。我们可以利用这个性质对数据降维：使用前面的小部分特征向量，丢弃掉那些包含没有方差的维度。这个操作也称为**主成分分析(PCA)**降维：
``` python
Xrot_reduced = np.dot(X, U[:, :100])
```
经过上面的操作，将原始的数据集大小由[N x D]降维到[N x 100]，留下的数据中包含最大方差的100个维度。通常使用PCA降维的数据作为线性分类器和神经网络的输入数据表现的效果会更好，同时还能节省时间和存储器空间。

最后一个实践是**白化**，是在特征基准上的数据，然后对每一个维度除以其特征值来对数值范围进行归一化。几何解释是，如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将是一个均值为0，协方差不变。操作如下：
``` python
# whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
Xwhite = Xrot / np.sqrt(S + 1e-5)
```

*噪声过大*，我们对数据加上了`1e-5`来防止除0的操作，但是可能会导致数据中的噪声被过分放大，因此可以采用更强的平滑来解决，比如采用更大的值。

**实际使用中**，卷积神经网络并不会使用PCA/白化的操作，只是作为补充内容。但是*零对称差分*和*归一化*数据是很重要的，也是很常见的操作。

>**对数据进行预处理时，切勿将训练集，验证集和测试集混为一谈，统一进行预处理。要将三个数据集分开分别进行数据的预处理操作。**

### 权重初始化
在介绍构建网络之前，先介绍对参数权重的初始化的方法：
1.  **全零初始化**是一种错误的初始化方法，如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而在每个神经单元都进行同样的参数更新，也就是说权重被初始化为同样的值，神经元之间就失去了*不对称*的源头。
2.  **小随机数初始化**仍然希望初始的权重接近0，但不等于0。因此，通常使用小的随机数进行初始化来打破对称性，这样神经元就不会进行同样的参数更新，而是各自进行各自的参数更新。实现方法通常类似于`W = 0.01 * np.random.randn(D, H)`，该方法能够产生零均值的高斯分布数据。同样，也可以使用均匀分布产生的随机数，从实践结果来看，这些方法产生随机化初始值的结果效果差不多。
> 并不是小数值会得到更好的结果。例如，一个神经网络的层中的权值很小，那么在反向传播的时候计算得出的梯度也非常小，这样就减小了反向传播中的'梯度信号'，在深度网络中容易出现问题。
3.  **使用1/sqrt(n)校准方差**是为了解决随着数据量的增长，随机产生的初始化权重分布的方差也会增大。除以输入数据量的平方根来调整其数值范围，神经元的输出方差就能够归一化到1了。也就是使用`W = np.random.randn(n) / sqrt(n)`进行初始化的效果会更好，能够提高收敛速度。但是在ReLU神经元的特殊初始化可使用`W = np.random.randn(n) * sqrt(2.0 / n)`，该形式是神经网络算法使用ReLU神经元的最佳推荐。
4.  **稀疏初始化**将所有的权重设置为0，但是每个神经元都同下一层固定数目的神经元随机连接用来打破对称性，一般典型的连接数目是10个。

偏置的初始化方法：由于权重的初始化已经打破了对称性，偏置通常可以设置成0。对于ReLU非线性激活函数，有人喜欢使用0.01这样比较小的常量作为所有偏置的初始值，这是因为他们认为这样的初始化能够使得ReLU单元一开始就激活，能够保存并传播一些梯度。但是实践证明有些实验结果反而会更差，因此还是推荐使用0进行初始化。
批量归一化：让激活函数在训练开始前通过一个网络，网络处理数据使其服从高斯分布。实际运用中，通过在全连接层(或者是卷积层)和激活函数之间添加一个BatchNormal层来实现。实践证明使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。批量归一化在每一层之前都做预处理，而且这种操作与网络集成在一起。

### 正则化
下面讨论防止神经网络过拟合的方法：
1.  **L2正则化**是最常用的正则化方法。可以通过惩罚目标函数中所有的参数平方来实现，即对网络中的每个权重$w$，向目标函数中增加$\frac{1}{2}\lambda w^2$的项，其中$\lambda$是正则化强度。之所以使用因子$\frac{1}{2}$是因为在反向传播计算$w$的梯度时简单的使用$\lambda w$而不是$2\lambda w$。L2正则化可以直观的理解为对大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。因此，是网络能够更加倾向于以来所有的输入特征，而不是严重的依赖于某一小部分特征。在梯度下降和参数更新的时候，使用L2正则化，所有的权重都会以`W += -lambda * W`的方式线性下降趋向于0。
2.  **L1正则化**是另一个很常用的方法。类似于L2正则化，不过L1正则化是向目标函数中增加$\lambda |w|$的项。L1正则化和L2正则化可以进行结合：$\lambda_1|w| + \lambda_2w^2$([Elastic net regularization](http://web.stanford.edu/~hastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&%20Hastie.pdf))。
>L1正则化能够让权重向量在最优化的过程中变得稀疏(即非常接近0)，也就是说使用L1正则化的神经元只使用重要输入的一部分稀疏子集，而且对噪声有更强的鲁棒性。L2正则化则会让权重变得发散，并且是较小的数值。实际上，如果特征选择并不是很重要，那么L2正则化比L1正则化会更好。
3.  **最大范数约束(Max Norm Constraints)**是给每个神经元权重向量的量级设置上限，并使用投影梯度下降来确保这一约束。在实践中，参数的更新按照正常方式进行，然后对权重向量$\overrightarrow{w}$做以下约束：$||{\overrightarrow{w}}||_2 < c$，通常$c$的值为3或者4。有些实验者提出这种正则化的方式会有很好的性能，并且能够防止网络中出现数值'爆炸'的情况，这是因为参数的数值始终被限制着。
4.  **随机失活(Dropout)**是一个简单又极其有效的正则化方法。该方法在[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)论文中提到。在训练过程中，随机失活的实现方法是让神经元以超参数$p$为概率被激活或者设置成0，示意图如下：
{% asset_img dropout.jpg %}
照片来源于论文，在训练过程中，随机失活可以被认为是对完整神经网络抽样出一些子集，每次基于输入数据只更新子集中的参数。但是在测试过程中，并不使用随机失活。随机失活的代码可以参考以下部分：
``` python
""" Vanilla Dropout: Not recommended implementation (see notes below) """
p = 0.5 # probability of keeping a unit active. higner = less dropout

def train_step(X):
    """ X contains the data """

    # forward pass for example 3-Layer neural network
    H1 = np.maximum(0, np.dot(W1, X) + b1)
    U1 = np.random.rand(*H1.shape) < p # first dropout mask
    H1 *= U1  # drop!!!
    H2 = np.maximum(0, np.dot(W2, H1) + b2)
    U2 = np.random.rand(*H2.shape) < p # second dropout mask
    H2 *= U2  # drop!!!
    out = np.dot(W3, H2) + b3

    # backward pass: compute gradients... (not shown)
    # perform parameter update... (not shown)

def predict(X):
    # ensembled forward pass
    H1 = np.maximum(0, np.dot(W1, X) + b1) * p # Scale the activations
    H2 = np.maximum(0, np.dot(W2, H1) + b2) * p
    out = np.dot(W3, H2) + b3
```
在上面的代码中，`train_step`函数中在第一个和第二个隐藏层中进行了两次随机失活。也可以在输入层进行随机失活。在`predict`函数中不进行随机失活，但是对两个隐藏层都要附带一个因子$p$，用来调整其数值范围。这个因子是非常重要的，因为在测试中，所有的神经元都是激活状态，如果要使输出和训练的时候保持一致，就要使用该因子。为了更好的理解，在训练的时候进行随机失活，那么输出为$px + (1 - p)0$，而在测试的时候所有神经元都是激活状态，那么输出就变成了$x$，因此加上因子$p$才能保证输出一致。
上面的代码在预测的时候需要进行数值的缩放，因此实际上更倾向于使用**方向随机失活(inverted dropout)**，只需要在训练的时候进行缩放，而在测试的时候不需要进行该操作。
``` python
"""
Inverted Dropout: Recommended implementation example.
We drop and scale at train time and don't do anything at test time.
"""

p = 0.5 # probability of keeping a unit active. higner = less dropout

def train_step(X):
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(*H1.shape) < p) /  # first dropout mask. Notice /p!
  H1 *= U1  # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!
  H2 *= U2  # drop!!!
  out = np.dot(W3, H2) + b3

  # backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)

def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3
```
更多相关Dropout请参考http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf 以及 http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf

**前向传播中的噪声**，在更一般化的分类上，随机失活属于网络在前向传播中有随机行为的方法。测试时，通过分析法（在使用随机失活的本例中就是乘以$p$）或者数值法（例如通过抽样出很多子网络，随机选择不同子网络进行前向传播，最后对它们取平均）将噪声边缘化。在这个方向上的另一个研究是`DropConnect`，它在前向传播的时候，一系列权重被随机初始化0。**偏置正则化**，对偏置参数的正则化并不常见，因为它们在军阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果。然而在实际应用中（使用了合理数据预处理的情况下），对偏置进行正则化也很少会导致算法性能变差。这可能是因为相较于权重参数，偏置参数实在太少，所以分类器需要它们来获得一个很好的数据损失，那么还是能够承受的。**每层进行正则化**，对不同的层进行不同强度的正则化很少见，关于这个思路的相关文献也很少。**实践**，通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也是很常见的。$p$值一般默认设置为0.5，也可能在验证集上调参。

### 损失函数
我们已经讨论过损失函数的正则化损失部分，它可以看作是对模型复杂程度的惩罚。损失函数的第二个部分是数据的损失，这是一个监督学习问题，用来衡量分类算法的预测结果(分类评分)和真实标签结果之间的一致性。数据损失是对所有的样本的数据损失求平均。也就是说：$L = \frac{1}{N}\sum_i{L_i}$中， $N$是训练集数据的样本数。让我们把神经网络中输出层的激活函数简写为$f = f(x_i; W)$，在实际中可能需要解决以下问题：
* **分类问题**：假设有一个装满样本的数据集，每个样本都有一个唯一的正确标签。在这类问题中，一个最常见的损失函数就是SVM：$L_i = \sum_{j \neq y_i}{max(0, f_j - f_{y_i} + 1)}$。有些人提出使用平方折页损失($\max(0, f_j - f_{y_i} + 1)^2$)会有更好的性能。第二个常用的选择是Softmax分类，使用交叉损失熵损失：$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)$。当标签非常多的时候，需要使用*分层Softmax(Hierarchical Softmax)*[参考文献](https://arxiv.org/pdf/1310.4546.pdf)。分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。
* **属性分类**：分类问题是每个样本只有一个正确的标签。但如果标签是一个二值向量，每个样本可能会有很多属性，也可能没有，属性之间并不会相互排斥。在这种情况下，一个明智的做法是为每个属性创建一个独立的二分类的分类器。例如，针对每个二分类器会采用下面公式：$L_i = \sum_i{\max(0, 1 - y_{ij}f_i)}$。上式中，求和是对所有分类$j, y_{ij}$的值根据第i个样本是否包含第j个属性确定为+1或者-1，评分向量$f_j$在分类正确的时候为正，否则为负。因此可以发现，当一个正样本评分小于+1或者一个负样本的评分大于-1的时候，损失就会被累加。另一种方法是对每种属性训练一个独立的逻辑回归分类器。
二分类的逻辑分类器只有两个分类(0, 1)，对于分类为1的概率为：
$$ P(y = 1 | x;w, b) = \frac{1}{1 + e^{-(w^Tx + b)}} = \sigma(w^Tx + b) $$
分类为0的概率就是：$P(y = 0 | x;w, b) = 1 - P(y = 1 | x;w, b)$。因此，假设一个样本被认定为是正样例要满足$\sigma(w^Tx + b) > 0.5$，也就是说$w^Tx + b > 0$。问题就可以简化为：
$$ L_i = \sum_j\Big({y_{ij}\log(\sigma(f_j)) + (1 - y_{ij})\log(1 - \sigma(f_j))\Big)} $$
上式中看起来很复杂，但是$f$的梯度实际上非常简单：$\partial{L_i} / \partial{f_j} = y_{ij} - \sigma(f_j)$
* **回归问题**：是预测实数的值的问题，比如预测房价、预测图片中物体的长度等。对于这个问题，通常是计算预测值和真实值之间的损失。然后用L2平方范数或L1范数度量差异。对于某个样本，L2范数计算如下：
$$ L_i = \Vert f - y_i \Vert_2^2 $$
使用L2范数的原因是梯度的计算会更简单，最优参数也不会改变是因为平方是一个单调运算。L1范数则是要将每个维度上的绝对值加起来：
$$ L_i = \Vert f - y_i \Vert_1 = \sum_j \mid f_j - {(y_i)}\_j \mid $$
如果有多个样本同时被预测，就要对预测所有样本进行求和。第i个样本的第j维数据，使用$\delta_{ij}$表示预测和真实值之间的不同，使用L2范数或者是$sign(\delta_{ij})$能够很好的计算梯度。也就是说，评分值的梯度要么与误差中的差值直接成比例，要么是固定的并根据正负的符号不同确定。注意，L2损失比较较为稳定的Softmax损失来说，其最优化过程要困难很多。直观而言，他需要网络具备一个特别的性质，即对于每个输入和增量都要输一个确切的正确值。而在Softmax中就不是这样，每个评分的准确值并不是那么重要：只有当它们量级适当的时候才有意义。另外，L2损失的鲁棒性不是很好，因为异常值会导致很大的梯度。当面对一个回归问题时，输出量转化成二值化时显然会出现不足的情况。例如，对产品进行评分时，一般我们可能使用5个独立的分类器来表示5个星级，而不是使用回归损失。分类器还能给出关于回归输出的分布，而不是一个简单的毫无把握的输出值。如果确信分类不适用，那么使用L2损失吧。
>当面对一个回归问题时，首先要考虑是不是必须要这样做。一般而言，尽量把输出变成二分类，然后对它们进行分类，从而变成一个分类问题。

## 实践
### 梯度检查
理论上，梯度检查就像是比较分析梯度和数值梯度，但这很容易出错。下面给出一些技巧：
1. **使用对称差分公式**：在计算梯度的时候，推荐使用下面公式：
$$ \frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in} \text{(use instead)} $$
该公式在检查梯度的每个维度的时候，会计算两次损失函数，因而会导致2倍的时间复杂度，但是会得到更为精确的值。
2. **使用相对误差进行比较**：对于比较数值梯度$f'_n$和分析梯度$f'_a$的方法，很容易能够想到使用两者之间差值的绝对值$\mid f’_a - f’_n \mid$是否大于一个规定的阈值来确定数值梯度是否接近分析梯度。然而，这是错误的，这是因为假设两者的差值为1e-4，如果两个梯度值在1.0左右时，这个差值时非常小的，但是如果两个梯度是1e-5或者更低的时候，那么这个梯度就是有问题的。因此，使用相对误差会更好一些：
$$ \frac{\mid f'_a - f'_n \mid}{\max(\mid f'_a \mid, \mid f'_n \mid)} $$
上式考虑了差值占两个梯度绝对值的比例，注意通常相对误差公式只包含两个式子中的一个，但是更倾向于取两个式子中最大的一个或者和。这样能够有效的防止分母为0的情况。然而还是要注意检查分母是否为0。通常，在实践中，相对误差大于1e-2表示两者差距是非常大的，大于1e-4也是稍微有些不妥的。小于1e-4对于有不可导点的函数来说算是可以的，但是小于1e-7来说才算是一个好的结果。
3. **使用双精度**： 一个常见的错误是使用单精度进行梯度检查，这样会导致即使梯度实现的正确，但是相对误差还是比较高的。
4. **浮点数据的范围**：最好通读["What Every Computer Scientist Should Know About Floating-Point Arithmetic"](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)一文，该文章主要讨论了使用浮点数据常见的错误，能够促使你更加细心的完成你的代码。例如，在神经网络中，对每个批量的数据的损失函数进行归一化是非常通用的。然而，如果每个数据点的梯度很小，然后还要除以批量的大小，那么所得的数值会更小，将会导致更多的问题。因此，可以在每一步都输出数值和分析梯度，方便进行比较，如果数值小于1e-10的话，那么梯度就太小了。如果数值确实过小，可以使用一个常数暂时将损失函数的数值范围扩展到一个更适合的范围，比较好的范围在1.0左右。
5. **目标函数的不可导点**：在梯度检查时，另一个导致数值梯度不准确的原因是不可导点问题。不可导点是指目标函数的不可导部分，是由ReLU、SVM损失函数以及Maxout神经元等引入。以ReLU函数为例，在$x = -1e6$的点，由于$x < 0$，所以分析梯度为0.然而当$f(x + h)$跨越不可导点的时候($h > 1e-6$)，计算出来的数值梯度将会不为0。你可能认为这是一个极端的情况，但事实上是非常常见的。例如，在CIFAR-10的SVM案例中，由于每个例子目标函数将会引入9个$\max(0, x)$的项，那么总共会引入超过450000个项。更为常见的，一个神经网络将会因为使用了ReLU激活函数产生更多的不可导点。
6. **使用少量的数据点**：对于上面不可导点的问题，一个方法是使用更少的数据点。因为当数据量越少，那么不可导点也会越少，所以在计算有限差值近似的时候越过不可导点的概率就会更小。另外，如果梯度检查对2-3个数据都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用少量的数据点，能让梯度检查更迅速有效。
7. **谨慎设置步长$h$**：步长并不是越小越好，这是由于当步长特别小的时候，就可能会约到数值精度问题。如果梯度无法进行检查的时候，那么可以试试将梯度调整为1e-4或者1e-6，可能就恢复正常了。具体可参考[wiki](https://zh.wikipedia.org/wiki/%E6%95%B8%E5%80%BC%E5%BE%AE%E5%88%86)。
8. **梯度检查的局部性**：一定要知道梯度检查是在参数空间随机的一个点上进行的，即使在那个点的梯度检查正确了，也不能保证在其他点都是正确的。另外，一个随机初始化的参数可能不是参数空间最优代表性的点，并可能会导致梯度看起来正确但实际不正确的极端情况。例如，SVM使用较小的数值作为权重的初始化，就会把一些接近于0的评分引入到数据点，而梯度在所有的数据点上都会呈现一种特殊的模型，并且不正确的梯度检查也可能会产生这种特殊的情况。因此就出现了看似正确实际却不正确的特殊情况。因此，为了安全起见，最好让网络学习预热一小段时间，等损失函数开始下降之后在进行梯度检查。
9. **不要过度规范化**：一般损失函数包括数据的损失和惩罚损失两个部分，一个比较容易出错的地方是惩罚损失过大，以至于梯度主要来自惩罚损失的部分。因此，推荐先仅对数据损失做单独的梯度检查，然后再对正则化损失进行单独的梯度检查。对于正则化的单独检查可以是修改代码，去掉数据损失部分，也可以提高正则化的强度，确认其效果再梯度检查中是否为可以忽略的，这样和容易看出来梯度检查的实现是否正确。
10. **不要使用Dropout和数据增强**：由于Dropout再反向传播实现中可能会出现错误，所以对梯度检查会有影响。对于很多不确定效果的操作，可能会对梯度检查产生不利的影响，无法定性的进行数值梯度和分析梯度进行比较，因此不使用这些操作才能够有效的进行梯度检查。另外，还有一个解决方案：在计算$f(x + h)$和$f(x - h)$之前，强制增加一个特定随机种子的项，并在验证分析梯度的时候也加入该随机种子，达到一致的目的。
11. **仅对少量的维度进行检查**：在实际应用中，参数可能有上百万个，这种情况下显然不适合对所有的参数都进行梯度检查。可以假设其他梯度是正确的，然后抽出来一部分进行梯度检查。在很多时间运用中，会习惯性的将所有的参数放到一个很大的向量中，这个时候偏置也是其中的一部分，这个时候就不要随机地从向量中选取维度，一定要考虑到偏置部分。

### 训练前一些合理性检查
* **确保损失正确**：一定要确认当你使用小的参数进行参数初始化时损失是正常的，最好是先单独的检查数据损失。
* 确保提高惩罚强度的时候，损失值会增大。
* **小数据集过拟合**：对整个训练集进行训练之前，尝试使用很小的数据子集进行训练，看损失是否能够接近0（注意不要加入惩罚损失，该项会防止损失为0）。如果能够使得最终的损失为0，那么对整个训练集才可能达到较好的效果。但是对小数据集过拟合并不代表是好的，依然有可能存在不正确的实现。例如，数据点的特征是随机的，这样算法也可能对小数据过拟合，但是放到整个数据集上的时候，就没有任何泛化能力。

### 训练过程
在对神经网络进行训练的时候，应该跟踪多个重要的数值，从而直到如何改变超参数以获得更高效的学习过程。下面讨论设计的图表中，大部分x轴都是以epoch为单位的，表示在训练中每个样本数据被训练过次数。
1. **损失函数**是最重要的指标，在前向传播中独立的对每个批次进行计算。下图给出了损失函数的曲线图，尤其是标注了学习率对于损失函数的影响：
{% asset_img learningrates.jpg %}
过低的学习率损失函数呈现线性，高一些的学习率看起来呈现几何指数下降，更高的学习率会让损失值很快的下降，但这会停到一个不是很好的点上。损失值的震荡程度和批量的大小有关，当每个批次数量为1的时候，震荡相对会大一些，批次大一些就好好点。
2. **训练集和验证的准确率**：在训练分类器的时候，第二个比较重要的指标是准确率。下面图表显示该曲线：
{% asset_img accuracies.jpg %}
蓝色验证集的曲线显示相较于训练集，验证集的准确度低了很多，这说明模型有很强的过拟合。遇到这种情况，就应该增大惩罚强度或者增大训练集。
3. **权重更新的比例**：最后一个应该追踪的量是权重中*更新值的数量和*全部值的数量之间的比例。需要对每个参数集的更新比例进行单独的计算和跟踪，一个经验性的结论是更新的比例大致是**1e-3**，如果更低，说明学习率设置的太小、如果更高，说明学习率设置的过高了。下面给出一个例子：
``` python
# assume parameter vector W and its gradient vector dW
param_scale = np.linalg.norm(W.ravel())
update = -learning_rate * dW  # simple SGD update
update_scale = np.linalg.norm(update.ravel())
W += update # the actual update
print update_scale / param_scale
```
4. **每层的激活数据和梯度分布**：错误的参数初始化会导致学习过程很慢，甚至完全阻止学习过程。这个问题可以通过输出网络中每层的激活数据和梯度分布柱状图来诊断。如果出现任何奇怪的分布，就可能会出现问题。例如，tanh的神经元中，激活数据应该在[-1, 1]区间内分布，如果神经元的输出都集中在0附近，或者集中在-1和1上，那么肯定就出现问题了。

### 参数更新
1. **随机梯度下方法**
  * **普通更新**：最简单的更新方式是沿着负梯度方向改变参数。假设参数向量`x`和其梯度`dx`，最简单的更新方式如下：其中超参数`learning_rate`是一个超参数，在整个验证集上，只要学习率设置足够低，那么就能保证损失的降低。
    ``` python
    # Vanilla update
    x = x - learning_rate * dx
    ```
    但是学习率一般会进行线性的衰减：$\epsilon_k = (1 - \alpha)\epsilon_0 + \alpha\epsilon_\tau$，其中$\alpha = \frac{l}{\tau}$，在$\tau$步之后，一般使$\epsilon$保持为常数。

  * **动量更新**：这种方式总能够在深度网络中得到更好的收敛速度。损失值可以理解为在山上的高度，势能$U = mgh$，因而$U \propto h$。使用随机数初始化参数相当于在某个位置设定了初始速度0，这样最优化的过程就可以看作是模拟参数向量在山上滚动的过程。作用于物体的力和能量的梯度之间存在关系($F = - \nabla U$)，那么力就表示了损失函数的负梯度。另外，$F = ma$，所以梯度与该物体的加速度是成正比的，因为这种理解下，梯度影响的是速度，然后在影响位置，而不是直接影响位置。基于这种理解，那么参数更新的方式如下：
    ``` python
    # Momentum update
    v = mu * v - learning_rate * dx # integrate velocity
    x = x + v #integrate position
    ```
    这里引入了一个初始化为0的变量`v`和一个超参数`mu`。这个变量被称为动量并不是很恰当，实际上在物理意义上，该参数与摩擦系数更为接近。该变量会减小速度，并降低系统的能量，因此可能会导致最终的结果不是最优的。当交叉验证的时候，该参数一般设置为0.5， 0.9和0.99。和学习率一样，该参数会随着时间不断变大。一个典型的设置是从0.5开始，随着训练的进行，将该值慢慢退火至0.99左右。
    更新公式描述如下：$$v \gets \alpha v - \epsilon \nabla_\theta\Big(\frac{1}{m}\sum_{i = 1}^{m}{L(f(x^{(i)}; \theta), y^{(i)}})\Big) $$ $$\theta \gets \theta + v$$

  * **Nesterov动量** 对凸函数具有更强的理论收敛保证，并且在实践中它也始终比标准动量稍微好一些。其核心思想是：当前的参数向量为`x`，对于标准的动量更新来说，我们知道动量的项(排除第二个梯度的项)是通过公式`mu * v`对参数向量的微调。因此，当要计算梯度的时候，可以将计算的大概位置`x + mu * v`看作‘预期值’。因此，计算`x + mu * v`的梯度而不是先前位置`x`的梯度会更有意义。相关代码如下：
    ``` python
    x_ahead = x + mu * v
    # evaluate dx_ahead(the gradient at x_ahead instead of at x)
    v = mu * v - learning_rate * dx_ahead
    x += v
    ```
    然而，实际上，人们更倾向于使用普通SGD或者之前普通动量更新一样简单的代码。通过对`x_ahead = x + mu * v`进行变量变换来实现。代码如下：
    ``` python
    v_prev = v # back this up
    v = mu * v - learning_rate * dx # velocity update stays the same
    x += -mu * v_prev + (1 + mu) * v # position update changes form
    ```
    更新公式描述如下：$$v \gets \alpha v - \epsilon \nabla_\theta\Big(\frac{1}{m}\sum_{i = 1}^{m}{L(f(x^{(i)}; \theta + \alpha v), y^{(i)}})\Big) $$ $$\theta \gets \theta + v$$
  Nesterov动量和普通动量更新的区别图：
  ![nesterov](nesterov.jpg)

2. **学习率退火**
  在深度学习训练阶段，让学习率随着时间退火通常是有帮助的：如果学习率一直很高，系统动能就会过大，参数向量就会无规律地跳动，不能够达到最优解。但是如果慢慢减小学习率，可能很长的时间都是在浪费计算资源，因为可能会有很长的时间都在无规律的跳动。若是快速的减小，就会导致学习率过小而无法达到最优解。通常，可以采用3种方式：
  * **随步数衰减**：每进行几个周期就根据一些因素降低学习率。典型的值是5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定一般是根据实际情况而定的，使用一个固定的学习率来进行训练的同时观察验证集的错误率，每当错误率停止下降的时候，就乘以一个常数来降低学习率。
  * **指数衰减**：$\alpha = \alpha_0 e^{-kt}$，其中$alpha_0, k$是超参数，$t$是迭代次数。
  * **1 / t衰减**：$\alpha = \alpha_0 / (1 + kt)$，其中$alpha_0, k$是超参数，$t$是迭代次数。
  实践中，随步数衰减的随机失活更受欢迎，因为使用了超参数（衰减系数和以周期为时间单位的步数）比$k$更具有解释性。当然，如果有足够的计算资源，完全可以让衰减更缓慢一些，只不过训练时间会更长。

3. **二阶近似方法**
  在深度学习的背景下，第二类比较常用的最优化方法是基于**牛顿法**的，其更新公式描述如下：
  $$x \gets x - {\[Hf(x)\]}^{-1}\nabla f(x)$$
  其中，$Hf(x)$是[Hessian矩阵](http://en.wikipedia.org/wiki/Hessian_matrix)，表示函数的二阶偏导数的平方矩阵。Hessian矩阵实际上描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。这个公式中没有学习率这个超参数，可以说是相对于一阶方法的一个优势。
  注意上面的牛顿法只适应于Hessian矩阵是正定的情况，在深度学习中，目标函数的表面通常非凸。因此使用牛顿法是有问题的。如果Hessian矩阵不是正定的，当靠近鞍点的时候，牛顿法实际上会导致更新沿着错误的方向进行。这种错误可以通过正则化Hessian矩阵来避免，更新公式仅仅是对Hessian矩阵在对角线上加上一个常数：
  $$x \gets x - {\[Hf(x) + \alpha I \]}^{-1}\nabla f(x)$$
  另一方面，对于计算Hessian矩阵以及求逆操作时间复杂度和空间复杂度都很大。通常采用的方法是BFGS算法和L-BFGS算法。因此在实践中，使用该方式的二阶方法并不常用，而基于动量更新的各种随机梯度下降算法更加常用，并很容易进行扩展。

4. **自适应学习率算法**
  * **AdaGrad**：由[Duchi等人](http://jmlr.org/papers/v12/duchi11a.html)提出。
    ``` python
    # Assume the gradient dx and parameter vector x
    cache = cache + dx ** 2
    x     = x - learning_rate * dx / (np.sqrt(cache) + eps)
    ```
    `cache`分别跟踪了所有参数的所有梯度历史平方值的总和。具有损失最大偏导数的参数应该相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。在凸优化的背景中，AdaGrad算法具有令人满意的效果，但是实践已经证明，从训练开始时积累梯度平方会导致有效学习率过早和过量的减小，以至于停止学习。
  * **RMSProp**：修改了AdaGrad以在非凸设定下效果更好，改变梯度累积为指数加权的移动平均。
    ``` python
    cache = decay_rate * cache + (1 - decay_rate) * dx ** 2
    x     = x - learning_rate * dx / (np.sqrt(cache) + eps)
    ```
    `decay_rate`是一个超参数，目前证明使用0.9，0.99或0.999具有良好的效果。参数`x`的更新和AdaGrad是相同的，但是`cache`是有所缩减的，因此不至于学习率快速的减小。
  * **Adam**：是最近提出来的一个算法，看起来是动量更新版本的RMSProp。更新方法如下：
    ``` python
    m = beta1 * m + (1 - beta1) * dx
    v = beta2 * v + (1 - beta2) * (dx ** 2)
    x = x - learning_rate * m / (np.sqrt(v) + eps)
    ```
    这个算法跟RMSProp很像，只不过是使用了平滑的梯度`m`，而不是原始的梯度向量`dx`。论文中推荐的超参数值为$eps = 1e-8, beta1 = 0.9, beta2 = 0.999$。在实际使用中，推荐使用该算法，一般而言比RMSProp更好一点。

  下图给出几种算法的效果比较：
  {% raw %}
  <div>
    <img src="opt2.gif" width="49%" style="display: inline-block">
    <img src="opt1.gif" width="49%" style="display: inline-block; border-left: 1px solid black;">
  </div>
  {% endraw %}
  更多的可以参考论文[Unit Tests for Stochastic Optimization](http://arxiv.org/abs/1312.6055)关于对随机最优化的测试的结论。

### 超参数调优
正如之前所将，在神经网络的训练过程中，会遇到很多超参数设置，最主要的参数包括：
* 初始学习率
* 学习率衰减方式
* 正则化强度
还有其他相对不那么敏感的超参数。下面给出一些额外的调参要点和技巧：
1. 更大的神经网络需要更长的实践去训练，调参可能需要几天甚至几周的时间。一个比较常用的是写一个子程序来不断的随机设置参数然后进行最优化。在训练过程中，该子程序在每个周期后对验证集的验证的过程中对准确率进行监控，然后保存到文件中，方便进行查找和排序。然后主程序负责启动或者结束子程序，有条件的话可以对子程序进行多线程集群，并能够可视化所有子程序的统计数据的结果。
2. 相对于交叉验证，**最好使用一个验证集**。在大多数情况下，一个尺寸合理的验证集可以保持代码的整洁，不需要用几个数据集进行交叉验证。
3. 超参数的范围：在对数尺度上进行超参数搜索。由于学习率和正则化强度对训练的动态进程有乘的效果，一个典型的学习率应该看起来为`learning_rate = 10 ** uniform(-6, 1)`。也就是说可以从标准分布中随机生成一个数字，然后让它成为10的阶数。对于正则化强度，可以采用相同的策略。当学习率为0.001的时候，如果对其固定的增加0.01，那么对于训练的影响会很大，但是如果学习率为10，那么影响就微乎其微了。但是一些参数如随机失活还是在原始尺度上进行搜索(`dropout=uniform(0.1)`)。另外，还要确认这个值不是出在范围的边界上，不然可能会错过更好的其他搜索范围。遇到边界上的值，可以通过扩大搜索范围来进行。
4. 分阶段搜索：在实践中，可以对整个范围先进性粗粒度的搜索。然后根据结果选定好结果出现的范围，将范围进行缩小，然后进行细粒度的搜索。在粗粒度搜索的时候，只需要让模型训练一个周期即可，因为很多超参数的设定会让模型无法学习，或者突然就爆出很大的损失值。细粒度的搜索时可以让模型训练5个周期。对粗粒度和细粒度的搜索可以划分更多的阶段。
5. 贝叶斯超参数优化主要研究在超参数空间中更高效的导航算法。其核心思想是在不同超参数设置下查看算法的性能，在探索和使用中进行合理的权衡。但是在卷积神经网络的实际使用中，该方法的性能还是差一点。

### 评估-模型集成
在训练的时候训练几个独立的模型，然后在测试的时候平均它们的预测结果总能够提高神经网络的几个百分点。但是随着模型数量的增加，算法的结果提升的空间也越来越小。另外，模型之间的差异越大，提升效果可能会越好。进行集成有以下几种方法：
* 同一个模型，不同的初始化：使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。风险在于多样性只来自不同的初始化条件。
* 在交叉验证中发现最好的模型：使用交叉验证来得到最好的超参数，然后取其中最好的几个模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。
* 一个模型设置多个记录点。如果训练十分耗时，那么就在训练周期结束后记录下多个记录点，然后用它们进行模型的集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，而且有训练代价小的优势。
* 在训练的时候计算参数的平均值。在训练过程中，如果损失值相较于之前的权重出现指数下降时，就在内存中对网络的权重进行一个备份。这样就可以对之前的几次循环中的网络模型的权重求平均值了。实践证明“平滑”的权重模型总是能够得到更小的误差。

通过上面的介绍，可以看出模型集成是一个很耗时的工作，最近Geoff Hinton在[Dark Knowledge](https://www.youtube.com/watch?v=EK61htlw8hY)中提出：通过将集成似然估计纳入到目标函数中，从一个好的集成中抽出一个单独的模型。
