线性分类
=======

## 介绍
接下来我们将会介绍线性分类的相关知识，对比于kNN算法，将会更加接近神经网络和卷积神经网络。该部分由两个部分构成：一部分将原始数据转化成相应分类分值的**评价函数**，另一部分是将预测的分值和真实值进行量化的**损失函数**。因此可以将问题转化成把损失函数最小化的优化问题。

## 从图片到标签分值的参数化映射
这个部分的主要方法是定义一个把图像中每个点的值转化成每个类别的置信评分的**评价函数**。我们假设一张训练图片\\(x_i\in R^D\\),其标签为\\(y_i\\)。 其中(\\( i = 1...N, y_i \in 1..K \\))，表明我们有**N**个训练图片，每张图片的维度为**D**，共有**K**个不同的分类。我们以**CIFAR-10**为例，训练集共有**N** = 50000张图片，每张图片的共有**D** = 32 x 32 x 3 = 3072个像素点，共有**K** = 10个分类。下面我们将会给出从原始图像的像素到分类评分的映射函数\\(f: R^D \mapsto R^K\\)：

**线性分类**，我们首先给出线性分类的简单函数：
$$ f(x_i, W, b) = Wx_i + b $$
在上面的等式中，我们假设图片\\(x_i\\)将所有的像素点展成一个单列的[D x 1]的向量。矩阵**W**(大小为K x D)，以及向量**b**(大小为[K x 1])作为函数的**参数**。应用到CIFAR-10的例子上，那么我们将会把第i张图片展成[3072 x 1]大小的单列向量，**W**大小为[10 x 3072]以及**b**的大小为[10 x 1]。因此将图像的原始像素输入到上述的函数中，那么将会输出10个数，作为分类的评分。参数**W**被称为**权重**，**b**被称为**偏置向量**(因为偏置影响最终的输出结果，但是跟图像的原始数据没有关系)。
这里有几点需要说明：
* 单一矩阵与\\(x_i\\)相乘的结果\\(Wx_i\\)能够并行的评估10个分类，每个分类的评估参数是**W**的一行。
* 输入数据\\((x_i, y_i)\\)是给定的并且是固定的，但是我们能够控制参数**W，b**。我们的目标是让训练数据计算得到的类的评分能够尽可能的和真实的标签相同。
* 这个方法的优点是可以使用训练数据去学习**W,b**，一旦学的参数，我们可以将学的参数保存起来，那么预测的时候只需要使用上述公式即可，只需要进行一次乘法和加法，计算是非常快的。
> 卷积神经网络也是类似的将图片的像素转成评分，但是映射的函数更为复杂并且包含更多的参数。

## 线性分类的解读
线性分类使用权重计算所有像素点的三个通道数值的和。依赖于精确的权重，函数能够挑选出在图片固定位置的一些颜色跟分类之间的联系。你可以想象船的周围是一些蓝色的，那么对于船的分类来说，在蓝色通道中会有很多正的权重，同样在红色或者绿色通道就会少一些。下图给出映射的一个例子：
![imagemap](linear_classification/imagemap.jpg)

### 将图像类比于高维的点
由于图片被拉伸成高维的列向量，我们可以将每张图片看成空间中的一个点，(例如CIFAR-10中的每张32 x 32 x 3像素的图片看成3072维上的一个点)。那么整个数据集就可以看成是一系列的点。介于我们为每个分为定义计算图片像素上权重的和的评分，每个分类的评分在空间上都是一个线性函数。我们无法直观的可视化3072维的空间，但是如果将这些维度挤压成2维的，那么就能尝试的可视化分类了，可视化的结果如下：
![pixelspace](linear_classification/pixelspace.jpg)
> 正如上图，每张图片看成是空间上的一个点，这里可视化了3个分类。红色的是车的类别，红色的先表示在车的分类上评分为0，红色的箭头表示评分的增加的方向，因此所有在红线的右侧能够得到正数的评分，左侧会得到负数的评分。

<hr />

正如上面我们所看到的，**W**的每一行对应着一个分类。这些数字的几何解释是，当我们改变**W**中的一行数据时，相应的在像素空间将会按照不同的方向进行旋转。偏置**b**在另一方面能够是我们的分类器按照每行进行划分分类。特别的，假设没有偏置项的话，如果输入的**\\(x_i = 0\\)**，那么无论权重是多少，输出都将为0，这就是偏置所能起到的作用。

**线性分类作为模板匹配的解释**：另一种对权重的解释时，对**W**的每一行，对应一个分类的模板(有的时候也称为原型)。一张图片的每个分类的评分通过使用每个模板和图片的内积(点乘)去寻找最适合的那个模板。按照这种术语来说，线性分类就是在做将学来的模板进行匹配的工作。另一种方式帮助我们理解的方式，把这种方式还是看作是最近邻，但不是使用上千张训练图片，我们仅仅每个类使用一张分类图片(这是我们学到的一个权重，仅仅是将该权重看作是一张图片，并非训练集中的一张图片)，我们使用内积的作为距离，而不是之前使用的L1或者L2距离。

<hr />

![templates](linear_classification/templates.jpg)
> 正如之前叙述的，这是我们从CIFAR-10中学到的权重。可以看出对于船模板正如所期望的包含很多蓝色的像素。这个模板将会对海上的一个船的图片给出很高的评分。但对于马的分类，看出有两个头，这是由于在训练集中，马的左脸和右脸都有，线性分类将这两种方式合并成一个模板。相似的，对于车的分类，看起来也是融合了多个图片的样式到一个模板中，从而能够从不同的方向识别出各种不同颜色的车。最终模板的颜色变成了红色，说明了在CIFAR-10中红色的车相较于其他颜色更多。因此线性分类可能识别其他颜色的车的准确率比较低，后面介绍的神经网络则会得到比较好的表现。

<hr />

### 偏置技巧
在继续下面的内容之前，我们将会使用一个常用的简单的技巧重写之前的函数，将参数**W, b**合并成一个，之前的函数如下：
$$ f(x_i, W, b) = Wx_i + b $$
在上面的公式中，权重**W**和偏置**b**是分开的，但是我们可以将这两个参数合并成一个矩阵，同样的将\\(x_i\\)多扩充一维并设置该值为1。那么上述的公式就可以写成下列的形式：
$$ f(x_i, W) = Wx_i $$
继续以CIFAR-10为例，\\(x_i\\现在大小为[3073 x 1]并非[3072 x 1] (加入了一个常数为1的多余的值)，参数**W**现在是[10 x 3073]而非之前的[10 x 3072]，**W**中多余的一行就是偏置**b**，下图给出了一个例子。
![wb](linear_classification/wb.jpg)

### 图像数据处理
上面的例子都是使用原始的像素的数据，值的范围是**[0, 255]** 。在机器学习中，一个常见的技巧是将输入特征正规化（在这个例子中，图片中的每个像素都可以看成一个输入特征）。特别的，将数据进行**中心化**是非常重要的，中心化的数据将会分布在**[-127, 127]** 。接着将数据进行缩放值**[-1, 1]**范围内。零作为均值是非常重要的，在接下来的随机梯度下降将会更为详细的介绍这部分内容。

## 损失函数
>**损失函数**也被称作**代价函数**或**目标**，当分类效果差的时候，损失函数较大，相反，当分类较好的时候，损失函数就变得很小。

### 多分类支持向量机损失
支持向量机的主要思想是将正例和反例之间的边界隔离(\\(\Delta\\))最大化。支持向量机损失希望正确的分类能够较错误的分类能够得到更高的固定的\\(\Delta\\)分值。那么假设给定一张图片\\(x_i\\)以及标签\\(y_i\\)，我们记分类的评分\\(f(x_i, W)\\)为\\(s\\)，例如第\\(i\\)张图片的第\\(j\\)个分类的评分**\\(s_j = {f(x_i, W)}_j\\)**。那么支持量机损失的第\\(i\\)个元素的损失为
$$ L_i = \sum_{j \neq y_i}{\max(0, s_j - s_{y_i} + \Delta)} $$
> 例如，我们计算得到分类的评分为 **\\(s = [13, -7, 11]\\)**，第一个分类为真实的分类，即 **\\(y_i = 0\\)**。假设 **\\(\Delta = 10\\)**，那么损失为 **\\(L_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)\\)**，最终的结果为8，与期望的边界10仅仅相差了2，这也说明了当错误分类时，损失函数至少是\\(\Delta \\)。

由于\\(f(x_i; W) = Wx_i\\)，因此我们继续可以将上式进行重写为：
$$ L_i = \sum_{j \neq y_i}{\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta)} $$
\\(w_j\\)表示参数**W**的第\\(j\\)行转化成列向量。

> 我们经常将阈值为0的 **\\(\max(0, -)\\)**称为**hinge损失**。实际应用中经常使用平方hinge损失(L2-SVM)，使用 **\\({max(0, -)}^2\\)** 的形式使得正例和反例之间的界限更加清晰。下图给出了支持向量机损失的思想的图片：
![margin](linear_classification/margin.jpg)

<hr />

### 正则化
上面给出的损失函数有一个问题，假设我们有一个数据集以及一系列的参数**W**能够正确的分出每个例子。问题是**W**并不是独一无二的，而是存在很多相似的**W**能够正确的分出这些数据。一个简单的例子是，加入对于每个例子，得到的损失都是0， 那么对于任意的\\(\lambda > 1\\)，所有的例子和参数\\(\lambda W\\)的乘积都是0。换句话说，我们希望通过一些变换能确定权重**W**来消除这些歧义。因此我们将损失函数使用**正则化的惩罚(regularization penalty) R(W)**进行规范。最常用的正则化惩罚是**L2规范**，它通过对所有的参数进行二次惩罚来使权重更加接近原点：
$$ R(W) = \sum_k{\sum_l{W_{k, l}^2}} $$
> 正则化函数是对权重进行规范，而非对原始数据进行规范

通过上式我们可以看出，我们将权重**W**中所有的元素的平方项进行求和。参数范数惩罚的支持向量机损失将会由两个部分构成，一部分是**原始数据的损失**，另一部分是**正则化的损失**。那么损失函数就变成了：
$$ L = \underbrace{\frac{1}{N} \sum_i L_i}_ {原始数据损失} + \underbrace{\lambda R(W)}_ {正则化损失} $$
展开上式便得到：
$$ L = \frac{1}{N}\sum_i{\sum_{j \neq y_i}{[\max(0, {f(x_i; W)}_j - {f(x_i; W)}_{y_i} + \Delta)]}} + \lambda\sum_k{\sum_lW_{k,l}^2} $$
N是训练集的大小，我们将正则化范数惩罚追加到损失中，并使用超参数$\Delta$控制权重，该超参数的确定并不容易，一般都是由交叉验证来确定。
惩罚过大的权重能够有效的促进扩散，这是因为输入的维度无法过多的影响最终的的评分。例如，假设输入向量$x = [1, 1, 1, 1]$，有两个权重的向量分别是$w_1 = [1, 0, 0, 0]$和$w_2 = [.25, .25, .25, .25]$。那么有$w_1^Tx = w2^Tx = 1$，两个权重将会有相同的结果。但是加入L2惩罚后，$w_1$的惩罚损失为1.0，而$w_2$的惩罚损失为0.25，因此最终导致$w_2$的损失比$w_1$的损失小。同样，也说明了L2惩罚倾向于更具有扩散性的权重，这也就意味着更偏向于让分类器考虑到大部分的输入而并不是集中在输入数据中的个别部分。这种扩散的效应能够**有效的防止过拟合**现象的发生。
>但是对于**偏置**项而言，它并不受输入数据的影响，因此我们没有并要去正则化范数惩罚偏置。但是在实际运用中，正则化范数惩罚偏置也只是对结果有微不足道的影响而已。另外，由于正则化损失的存在，最终的损失永远不会为0，这是由于权重不会为0，那么正则化损失也就不为0。

<hr />

### 相关代码
``` python
def L_i(x, y, W):
    """
    unvectorized version. Compute the multiclass svm loss for a single example (x, y)
    - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)
      with an appended bias dimension in the 3037-rd position (i.e. bias trick)
    - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)
    - W is the weight martix (e.g. 10 x 3073 in CIFAR-10)
    """
    dalta = 1.0       # see notes about delta later int this section
    scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class
    correct_class_score = scores[y]
    D = W.shape[0]    # number of classes, e.g. 10
    loss_i = 0.0
    for j in range(D):
        if j == y:
            # skip for the true class to only loop over incorrect classes
            continue
        # accumulate loss for the i-th example
        loss_i += max(0, scores[j] - correct_class_score + delta)
    return loss_i

def L_i_vectorized(x, y, W):
    """
    A faster half-vectorized implementation. half-vectorized refers to the
    fact that for a single example the implementation contains no for loop,
    but there is still one loop over the examples(outside the function)
    """
    delta = 1.0
    scores = W.dot(x)
    # compute the margins for all classes in one vector operation
    margins = np.maximum(0, scores - scores[y] + delta)
    # on y-th position scores[y] - scores[y] canceled and gave delta,
    # We want to ignore the y-th position and only consider margin
    # wrong class.
    margins[y] = 0
    loss_i = np.sum(margins)
    return loss_i

def L(X, y, W):
    """
    fully-vectorized implementation:
    - X holds all the training examples as columns (e.g. 3073 x 50000 in CIFAR-10)
    - y is array of integers specifying correct class (e.g. 50000-D array)
    - W are weights (e.g. 10 x 3073)
    """
    delta = 1.0
    num_train = X.shape[0]
    scores = W.dot(X)

    # selct the i-th scores
    yi_scores = scores[np.arange(scores.shape[0]), y]

    # compute the margins for all classes in one vector operation
    margins = np.maximum(0, scores - yi_scores.T + delta)
    margins[np.arange(num_train), y] = 0
    loss = np.mean(np.sum(margins, axis=1))
    return loss
```

## 实际运用

### $ \Delta $的设置
$ \Delta $的值应该设置成多少，我们是否应该对该值进行交叉验证？实际的经验证明$ \Delta = 1.0 $对所有的情况都比较适用。超参数$ \Delta $和$ \lambda $看起来像是两个不同的超参数，但是它们的作用是相同的：让原始数据的损失和惩罚的损失进行均衡。权重**W**的大小会直接影响评分：当我们对权重**W**按比例进行缩小时，评分也会缩小；按比例增大时，评分也会增加。类别之间评分的边界在一些场景下将会由于权重的缩小或者放大变得没有意义。因此，我们要很好的去控制权重增大的上限（通过参数$ \lambda $）。

### 与二分类支持向量机之间的关系
之前所提及的二分类支持向量机的Loss可以写成：
$$ L_i = C\max(0, i - y_iw^Tx_i) + R(W) $$
C是一个超参数，$ y_i \in \{-1, 1\} $。上面的公式可以作为二分类支持向量机用于二分类的问题上。在上面的公式中$C$和$\lambda$在同一个因素上进行折衷，并且这两个变量间互为倒数关系$ C \propto {\frac{1}{\lambda}} $。

### 初始化的优化
CS231n课程将会使用无约束的原始形式优化目标，但很多时候在原理上会出现不可微分现象（例如max(x, y)在x=y的时候不能微分)，但是实际运行中不会出现这种问题，因为我们使用梯度去解决问题。

### 其他多分类支持向量机公式
另一个常用的形式是OVA(over-vs-all)的支持向量机，原理是将每个分类分别按照二分类的形式(其他分类都看作是背景)去训练。在实际上不常见的策略是AVA(all-vs-all)。上面我们给出的公式是<a href="https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999-461.pdf">Weston and Watkins 1999 (pdf)</a>的版本，这个版本想读与OVA更好，这是因为这个版本能实现原始数据的损失为0，但是OVA不能。

## Softmax分类
如果你听说过二分类逻辑回归分类，那么Softmax是该分类的多分类版本。在softmax分类中，映射函数$ f(x_i;W) = Wx_i $没有变化，但是我们使用没有标准化的对数几率并将Hinge损失换成**交叉熵损失**来表示每个类的评分：
$$ L_i = -\log{\frac{e^f_{y_i}}{\sum_j{e^{f_j}}} \hspace{0.5in} or \hspace{0.5in} L_i = -f_{y_i} + \log{\sum_j{e^f_j}}} $$
$f_j$表示第j个元素对应的评分。像之前的支持向量机，整体的损失是$L_i$的平均值以及和回归项$R(W)$的和。函数$f_j(z) = \frac{e^{z_j}}{\sum_k{e^{z_k}}}$被称为**softmax函数**：将真实的任意的评分压缩成0到1之间的向量。

### 信息论解释
真实分布$p$和估计分布$q$之间的交叉熵被定义为：
$$ H(p, q) = -\sum_x{p(x)\log{q(x)}} $$
softmax分类器所做的就是最小化在估计分类概率和真实分布之间的交叉熵，在这个解释中，真实分布就是所有概率密度都分布在正确的类别上。并且，既然交叉熵能够写成熵和相对熵$$ H(p, q)= H(p) + D_{KL}(p||q)，并且delta函数$p$的熵是0，那么就能等价的看作是对两个分布之间的相对熵做最小化。换句话说就是交叉熵损失函数想要预测分布的所有概率密度都在正确的分类上。

### 概率论解释
softmax的公式可以看作是对于图片$x_i$，以$W$为参数，分配给正确分类标签$y_i$的归一化概率。softmax分类器将输出向量$f$中的评分解释为没有归一化的对数概率。那么以这些数值做指数函数的幂就得到了没有归一化的概率，而除法操作则对数据进行归一化处理，使得这些概率和为1.从概率论的角度来理解，我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计(MLE)。这种解释的一个好处是，损失函数中的正则化部分$R(W)$可以被看作是权重矩阵$W$的高斯先验，这里进行的是最大后验估计(MAP)而不是最大似然估计。

### 实际经验：数值稳定性
当你实际在写Softmax的函数的时候，中间项$e^{f_{y_j}}$和$\sum_j{e^{f_j}}$可能会由于指数运算变得非常大。被一个很大的数除可能在数值上变得很不稳定，因此使用归一化的技巧变得非常重要。假设我们同时将分子和分母同时乘以一个常数$C$，那么我么会得到下面等式：
$$ \frac{e^{f_{y_i}}}{\sum_j{e^{f_j}}} = \frac{Ce^{f_{y_i}}}{C\sum_j{e^{f_j}}} = /frac{e^{f_{y_i} + \log C}}{\sum_j{e^{f_j} + \log C}} $$
我们能够自由选择$C$的值，但是我们的目标是保证数值的稳定。一个通常的选择是$\log C = -\max_j{f_j}$。也就是说我们保证向量$f$的最大值为0。代码如下：
``` python
f = np.array([123, 456, 789])
p = np.exp(f) / np.sum(np.exp(f))   # bad, numberic problem, potential blowup.

# insteadL first shift the value of f so that the highest number is 0.
f -= np.max(f)
p = np.exp(f) / np.sum(np.exp(f))
```

### 令人疑惑的命名规则
精确的说，SVM分类器使用折页损失(hinge loss)，有的时候也称为最大边界损失(max-margin loss)。softmax分类器使用交叉熵损失(cross-entropy loss)。Softmax分类器的命名是从softmax函数那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说'softmax损失(softmax loss)'是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。

## SVM和Softmax的对比
下图能够说明SVM和Softmax的对比：
![svmvssoftmax](linear_classification/svmvssoftmax.png)

### Softmax分类器为每个分类提供了'可能性'
SVM的计算是无标定的，而且很难针对所有分类的评分给出直观的解释。Softmax则不同，允许对每个分类计算可能性。例如，给定一张图片，SVM给出的评分可能是 [12.5, 0.6, -23.0]，softmax则计算这三个分类的“概率”为[0.9, 0.09, 0.01]，能够直观的反应每个分类的可能性。对于概率为什么要加引号，这是由于概率分布的集中或者离散程度由正则化参数$\lambda$直接决定。假设非归一化的对数概率为[1, -2, 0]，Softmax计算的结果为：
$$ [1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26] $$
上述运算通过了指数变换，归一化步骤。当正则化参数$\lambda$的值比较大时，权重$W$将会被惩罚的更多，就会导致权重$W$更小。例如，假设上述的权重变成之前的一半，那么Softmax计算的结果如下：
$$ [0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33] $$
看起来概率分布更加分散了。随着正则化参数$\lambda$的不断增加，权重会越来越小，最后输出的概率接近均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的置信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但是具体的值是难以直观解释的。


### 在实际运用中， SVM和Softmax经常是类似的
SVM和Softmax之间的性能差别是非常小的，而且不同人对分类器的性能有着不同的评判。
>相对于Softmax分类器，SVM更具有*局部目标化(local objective)*，这可以看作优势也可以看作是劣势。
考虑到一个评分[10, -2, 3]，其中第一分类是正确的，那么一个SVM($\Delta = 1$)的正确分类相较于错误分类已经得到了比边界值还要高的分数，那么就会认为损失为0。SVM对数字个体的细节是不关心的：如果分数是[10, -100, -100]或者是[10, 9, 9]，对于SVM来说没有什么不同，只要满足超过边界值1，那么损失就是0。
但是对于Softmax来说，[10, 9, 9]的损失明显要比[10, -100, -100]高。换句话说就是Softmax分类对于评分永远不会满意，正确的分类总能够得到更高的概率，错误的分类总能得到更低的概率，损失值总是能够更小。

## 总结
* 定义了从图像像素到不同类别**评分函数**，这里使用了基于权重$W$和偏置$b$的线性函数。
* 不像kNN分类器那样，**参数逼近**是一旦学习到参数后，训练数据就可以被丢弃。同时，测试能够更快，这是由于我们只需要一个简单的参数$W$矩阵，而不需要和训练集一个一个进行比较。
* 通过**偏置技巧**能够是偏置向量写到参数矩阵$W$中，那么就只需要记住参数矩阵即可，而不需额外的存储偏置向量。
* 定义了**损失函数**，介绍了SVM和Softmax两个损失函数，来衡量参数的性能。这里就将预测分类问题转变成了损失最小化的问题。
